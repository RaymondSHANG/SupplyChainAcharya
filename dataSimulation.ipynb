{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 5 Day Gen AI: Agentic SupplyChain Acharya - Part I Creating A Digital Twin of a Small Supply Chain Network\n\n## Problem Statement: Building Intelligence into Retail Supply Chains\n\nIn modern **retail supply chains**, one of the most persistent challenges is **understanding and managing inventory levels**â€”specifically:\n\n-\tâŒ **Why are certain products out of stock?**\n-\tðŸ“¦ **Why there is too much of another product in-store?**\n\nThese issues contribute to:\n-\t**Customer dissatisfaction** due to poor shelf availability\n-\t**Operational inefficiencies** and increased costs from overstocking and waste\n---\n\n### Complexity of the Challenge\n\nStrategic, tactical, and operational decisions depend on **interconnected components**, such as:\n\n-\tðŸ­ **Suppliers**\n-\tðŸ¢ **Distribution Centers (DCs)**\n-\tðŸ¬ **Stores**\n-\tðŸ“† **Calendars controlling shipping/receiving/order review**\n-\tâš™ï¸ **Item-specific parameters that influence, Lead Times, Forecast and Replenishment Plan**\n\nEven seemingly minor misconfigurations in this ecosystem can result in **millions of dollars in inefficiencies**, particularly when scaled across large networks.\n\n---\n## Our Solution : ðŸ­ Supply Chain Acharya - A Supplychain AgentBot Combining Digital Twin + GenAI\n\nSupply Chain Acharya is a Gen AI-powered assistant designed to uncover these root causes dynamically to help the store managers and replenishment planners to determine the rootcause and recommend next steps. The functionality has been tested with the information using a Digital Twin of a retail supply chain network.  \n\n**We used this notebook to develope a Digital Twin** of a small supply chain network to simulate **30 days** of demand and replenishment activity.  \n## The Physical Network that we simulate includes:  \n-   ðŸ”¸ **2 Distribution Centers**  \n-\tðŸ”¸ **10 Suppliers**  \n-\tðŸ”¸ **30 Stores**  \n-\tðŸ”¸ **20 Items (2 items per supplier, per store)**  \n","metadata":{}},{"cell_type":"markdown","source":"```mermaid\ngraph TD;\n    SP1-->DC1;\n    SP2-->DC1;\n    SP3-->DC1;\n    SP4-->DC1;\n    SP5-->DC1;\n    SP6-->DC1;\n    SP7-->DC1;\n    SP8-->DC1;\n    SP9-->DC1;\n    SP10-->DC1;\n    SP1-->DC2;\n    SP2-->DC2;\n    SP3-->DC2;\n    SP4-->DC2;\n    SP5-->DC2;\n    SP6-->DC2;\n    SP7-->DC2;\n    SP8-->DC2;\n    SP9-->DC2;\n    SP10-->DC2;\n    DC1-->ST1;\n    DC1-->ST2;\n    DC1-->ST3;\n    DC1-->ST4;\n    DC1-->ST5;\n    DC1-->ST6;\n    DC1-->ST7;\n    DC1-->ST8;\n    DC1-->ST9;\n    DC1-->ST10;\n    DC1-->ST11;\n    DC1-->ST12;\n    DC1-->ST13;\n    DC1-->ST14;\n    DC1-->ST15;\n    DC2-->ST16;\n    DC2-->ST17;\n    DC2-->ST18;\n    DC2-->ST19;\n    DC2-->ST20;\n    DC2-->ST21;\n    DC2-->ST22;\n    DC2-->ST23;\n    DC2-->ST24;\n    DC2-->ST25;\n    DC2-->ST26;\n    DC2-->ST27;\n    DC2-->ST28;\n    DC2-->ST29;\n    DC2-->ST30;","metadata":{}},{"cell_type":"markdown","source":"# All 10 Suppliers Deliver to Both the DCs\n```mermaid\ngraph TD;\nA(( SP1))-- O -->DC1;\nB(( SP2))-- O -->DC1;\nC(( SP3))-- O -->DC1;\nD(( SP4))-- O -->DC1;\nE(( SP5))-- Supplier to DC -->DC1;\nF(( SP6))-- O -->DC1;\nG(( SP7))-- O -->DC1;\nH(( SP8))-- O -->DC1;\nI(( SP9))-- O -->DC1;\nJ(( SP10))-- O -->DC1;\nA(( SP1))-- O -->DC2;\nB(( SP2))-- O -->DC2;\nC(( SP3))-- O -->DC2;\nD(( SP4))-- O -->DC2;\nE(( SP5))-- Supplier to DC -->DC2;\nF(( SP6))-- O -->DC2;\nG(( SP7))-- O -->DC2;\nH(( SP8))-- O -->DC2;\nI(( SP9))-- O -->DC2;\nJ(( SP10))-- O -->DC2;","metadata":{}},{"cell_type":"markdown","source":"# However, Only DC1 Deliver to Stores 1 to 15\n```mermaid\ngraph TD;\n\nDC1-- o --> ST1((ST1));\nDC1-- o --> ST2((ST2));\nDC1-- o --> ST3((ST3));\nDC1-- o --> ST4((ST4));\nDC1-- o --> ST5((ST5));\nDC1-- o --> ST6((ST6));\nDC1-- o --> ST7((ST7));\nDC1-- DC to Store Product Flow --> ST8(( ST8));\nDC1-- o --> ST9((ST9));\nDC1-- o --> ST10((ST10));\nDC1-- o --> ST11((ST11));\nDC1-- o --> ST12((ST12));\nDC1-- o --> ST13((ST13));\nDC1-- o --> ST14((ST14));\nDC1-- o --> ST15((ST15));\n","metadata":{}},{"cell_type":"markdown","source":"# And Only DC2 Deliver to Stores 16 to 30\n```mermaid\ngraph TD;\n\nDC2-- o --> ST16((ST16));\nDC2-- o --> ST17((ST17));\nDC2-- o --> ST18((ST18));\nDC2-- o --> ST19((ST19));\nDC2-- o --> ST20((ST20));\nDC2-- o --> ST21((ST21));\nDC2-- o --> ST22((ST22));\nDC2-- DC to Store Product Flow --> ST23(( ST23));\nDC2-- o --> ST24((ST24));\nDC2-- o --> ST25((ST25));\nDC2-- o --> ST26((ST26));\nDC2-- o --> ST27((ST27));\nDC2-- o --> ST28((ST28));\nDC2-- o --> ST29((ST29));\nDC2-- o --> ST30((ST30));","metadata":{}},{"cell_type":"markdown","source":"# Simulating the Network to Create the Digital Twin\n-\tðŸ”¸ **Simulated parameter values**  \n    - System lead times (anticipated # of days from order generation to receipt at the store entered in replenishment system),  \n      - Further split into Vendor to DC Lead Times (V_DC_LT) and DC to Store Lead Times (DC_StoreLT)  \n      - Results in System Total Vendor Store Lead Time (STVSLT) as the sum of the two above\n      - We review and run the processes that creates orders every day and so we set review time (RT) to 1 day  \n      - Set Safety Stock(SS) to 1 Day of Supply, so OUTL (Order Up To Level) = (STVSLT + RT + SS days) * Forecast\n      - Calendars are assumed open everyday to reduce complexity  \n-  ðŸ”¸ **Simulated Replenishment(also called ERP or Order Management) Processes**\n   1. Simulated **demand forecast** for the 600 item store combinations:  \n      - We used one forecast generated on day1 of our planning period for the 30 days,   \n   2. Simulated **actual demand** drawn from a normal distribution with mean equal to the forecasted demand,  \n      - Simulated **sales** set as the smaller of Beginning inventory of the day and actual demand  \n   3. Simulated **Actual lead times** drawn from a normal distribution with mean equal to the System lead times\n      - Simulated both **V_DC_LT** and **DC_StoreLT** and added them to get **Total Actual lead times**\n   4. Day 1 - we started with the **forecast**\n      - Generated **Day 1 Inventory** based on the OUTL calculation explained mentioned above.\n        - Assumed we started selling after receiving the pre-order as above\n      - Generated **Sales** as per 2 above\n      - EOD inventory is estimated as BOD inventory - Sales\n   5. Day 2 to Day n Process Steps below\n      - BOD inventory is Yesterdays EOD inventory\n      - **1. Process Sales** Generate demand and set Sales to a maximum of the BOD inventory.\n        - Record in **StoreSales** table\n      - **2. Process Receipts** Check existing orders that are not yet recorded in the Receipt Table\n        - Based on Simulated **Total Actual Lead Times** determined as explained above,\n          - calculate **orderdate + Total Actual lead times** to estimate the arrival date.\n          - If the arrival date is less than the processing day record Received dt and Quantity in **StoreReceipts** table\n      - **3. Process Inventory** Yesterday's EOD is set as Day n BOD; Day n EOD = BOD - Sales + Received Quantity\n        - Record BOD and EOD inventory in the **InventoryOH** table\n      - **4. Process Orders** \n        -  Steps\n        - Caculate OUTL = (LT + RT + SSdays) * FC\n        - Calculate OP  = (LT + 0.5 * RT + SSdays) * FC\n        - OH = EODQty\n        - OO = Orderqty for all outstanding orders -  even if ordered today it is expected to be delivered within LT+RT window\n        - Calculate AvailableInv = OH + OO\n        - Apply Rule: Create an Order = if AvailableInv < OP , (OUTL - AvailableInv) else 0\n          - Record Orders in the **orders** table","metadata":{}},{"cell_type":"markdown","source":"# Digital Twin - Implementation Code","metadata":{}},{"cell_type":"markdown","source":"## Step-1 : Setup Environment and Read Gemini API Key Through Kaggle Secrets","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-04-20T04:32:10.183823Z","iopub.execute_input":"2025-04-20T04:32:10.184395Z","iopub.status.idle":"2025-04-20T04:32:10.523185Z","shell.execute_reply.started":"2025-04-20T04:32:10.184368Z","shell.execute_reply":"2025-04-20T04:32:10.522485Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab  # Remove unused conflicting packages\n!pip install -U -q \"google-genai==1.7.0\"","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:10.524795Z","iopub.execute_input":"2025-04-20T04:32:10.525232Z","iopub.status.idle":"2025-04-20T04:32:18.919753Z","shell.execute_reply.started":"2025-04-20T04:32:10.525211Z","shell.execute_reply":"2025-04-20T04:32:18.918259Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\ngenai.__version__","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:18.92114Z","iopub.execute_input":"2025-04-20T04:32:18.921478Z","iopub.status.idle":"2025-04-20T04:32:20.466647Z","shell.execute_reply.started":"2025-04-20T04:32:18.921453Z","shell.execute_reply":"2025-04-20T04:32:20.465777Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:20.467668Z","iopub.execute_input":"2025-04-20T04:32:20.468096Z","iopub.status.idle":"2025-04-20T04:32:20.671155Z","shell.execute_reply.started":"2025-04-20T04:32:20.468066Z","shell.execute_reply":"2025-04-20T04:32:20.670206Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define a retry policy. The model might make multiple consecutive calls automatically\n# for a complex query, this ensures the client retries if it hits quota limits.\nfrom google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n  genai.models.Models.generate_content = retry.Retry(\n      predicate=is_retriable)(genai.models.Models.generate_content)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:20.673727Z","iopub.execute_input":"2025-04-20T04:32:20.674262Z","iopub.status.idle":"2025-04-20T04:32:20.973866Z","shell.execute_reply.started":"2025-04-20T04:32:20.674239Z","shell.execute_reply":"2025-04-20T04:32:20.973064Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %sql sqlite:///SupplyChainABC2.db \n# %sql sqlite:///kaggle/working/SupplyChainABC2.db # to  write to Kaggle working directory - may be lost sometimes\n\n%load_ext sql\n\n%sql sqlite:///SupplyChainABC2.db \n\nimport pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:20.974672Z","iopub.execute_input":"2025-04-20T04:32:20.97505Z","iopub.status.idle":"2025-04-20T04:32:21.866129Z","shell.execute_reply.started":"2025-04-20T04:32:20.975028Z","shell.execute_reply":"2025-04-20T04:32:21.865431Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step-2 : Setup Database to Store Simulated Data and Write Functions to Manipulate Data","metadata":{}},{"cell_type":"code","source":"# Need to run this before execute query\n# db_file = \"/kaggle/working/SupplyChainABC2.db\"\n\n# connect using sqlite3\nimport sqlite3\ndb_file = \"SupplyChainABC2.db\"\n\n\n\ndb_conn = sqlite3.connect(db_file)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:21.867082Z","iopub.execute_input":"2025-04-20T04:32:21.867376Z","iopub.status.idle":"2025-04-20T04:32:21.872174Z","shell.execute_reply.started":"2025-04-20T04:32:21.867352Z","shell.execute_reply":"2025-04-20T04:32:21.871415Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Open a connection to a file-based database\n# backup_conn = sqlite3.connect(\"/kaggle/working/SupplyChainBackup.db\")\n\n# # Copy data from in-memory to file\n# with backup_conn:\n#     db_conn.backup(backup_conn)\n\n# # Close both connections\n# backup_conn.close()\n# db_conn.close()","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:21.872964Z","iopub.execute_input":"2025-04-20T04:32:21.873212Z","iopub.status.idle":"2025-04-20T04:32:21.889912Z","shell.execute_reply.started":"2025-04-20T04:32:21.873187Z","shell.execute_reply":"2025-04-20T04:32:21.889154Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define insert_df fn\ndef insert_dataframe_to_sqlite(df, db_name, table_name):\n    \"\"\"\n    Inserts data from a pandas DataFrame into a SQLite table.\n\n    Args:\n        df (pd.DataFrame): The DataFrame to insert.\n        db_name (str): The name of the SQLite database file.\n        table_name (str): The name of the table to insert into.\n    \"\"\"\n    conn = sqlite3.connect(db_name)\n    df.to_sql(table_name, conn, if_exists='append', index=False)\n    conn.close()","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:21.890921Z","iopub.execute_input":"2025-04-20T04:32:21.89125Z","iopub.status.idle":"2025-04-20T04:32:21.905917Z","shell.execute_reply.started":"2025-04-20T04:32:21.891222Z","shell.execute_reply":"2025-04-20T04:32:21.905102Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def execute_query(sql: str, debug=False) -> list[list[str]]:\n    \"\"\"Execute an SQL statement, returning the results.\"\"\"\n    if debug:\n        print(f' - DB CALL: execute_query({sql})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(sql)\n    return cursor.fetchall()\n\n\n# execute_query(\"select * from products\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:21.906794Z","iopub.execute_input":"2025-04-20T04:32:21.9078Z","iopub.status.idle":"2025-04-20T04:32:21.923206Z","shell.execute_reply.started":"2025-04-20T04:32:21.907776Z","shell.execute_reply":"2025-04-20T04:32:21.922347Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# If you want to recreate the tables run the cell below\nIF NOT COMMENT IT OUT","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\ndef drop_all_tables(db_file):\n    \"\"\"\n    Drops all user-defined tables in an SQLite database.\n\n    Args:\n        db_file (str): The path to the SQLite database file.\n    \"\"\"\n    conn = None\n    try:\n        conn = sqlite3.connect(db_file)\n        cursor = conn.cursor()\n\n        # Get a list of all user-defined tables\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\")\n        tables = cursor.fetchall()\n\n        if tables:\n            for table in tables:\n                table_name = table[0]\n                cursor.execute(f\"DROP TABLE IF EXISTS '{table_name}';\")  # Drop each table\n            conn.commit()\n            print(f\"All user-defined tables dropped from database '{db_file}'.\")\n        else:\n            print(f\"No user-defined tables found in database '{db_file}'.\")\n\n    except sqlite3.Error as e:\n        print(f\"An SQLite error occurred: {e}\")\n    finally:\n        if conn:\n            conn.close()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:21.924246Z","iopub.execute_input":"2025-04-20T04:32:21.924721Z","iopub.status.idle":"2025-04-20T04:32:21.94073Z","shell.execute_reply.started":"2025-04-20T04:32:21.924692Z","shell.execute_reply":"2025-04-20T04:32:21.939785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage:\ndb_file = \"SupplyChainABC2.db\"  # Replace with your database file if needed\ndrop_all_tables(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:21.941671Z","iopub.execute_input":"2025-04-20T04:32:21.941962Z","iopub.status.idle":"2025-04-20T04:32:21.963639Z","shell.execute_reply.started":"2025-04-20T04:32:21.941935Z","shell.execute_reply":"2025-04-20T04:32:21.962801Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Table Definitions","metadata":{}},{"cell_type":"code","source":"%%sql sqlite:///SupplyChainABC2.db\n\n-- Create the 'DC' table\nCREATE TABLE IF NOT EXISTS DC (\n    DC_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    DC_name VARCHAR(255) NOT NULL,\n    DCCity VARCHAR(255) NOT NULL\n  );\n\n-- Create the 'Store' table\nCREATE TABLE IF NOT EXISTS Store (\n    Store_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    Store_name VARCHAR(255) NOT NULL,\n    StoreCity VARCHAR(255) NOT NULL\n  );\n\n-- Create the 'Vendor' table\nCREATE TABLE IF NOT EXISTS Vendor (\n    \n    Vendor_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    Vendor_name VARCHAR(255) NOT NULL,\n    VendorCity VARCHAR(255) NOT NULL\n  );\n\n-- Create the 'VendorDC' table\nCREATE TABLE IF NOT EXISTS VendorDC (\n    Vendor_id INTEGER NOT NULL,\n    DC_id INTEGER NOT NULL,\n    Vendor_DC_LT INTEGER NOT NULL,\n    CONSTRAINT FK_VendorDC_Vendor FOREIGN KEY (Vendor_id) REFERENCES Vendor(Vendor_id),\n    CONSTRAINT FK_VendorDC_DC FOREIGN KEY (DC_id) REFERENCES DC(DC_id),\n    CONSTRAINT UQ_VendorDC UNIQUE (Vendor_id, DC_id)\n  );\n\n-- Create the 'Item' table\nCREATE TABLE IF NOT EXISTS Item (\n    Item_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    Item_name VARCHAR(255) NOT NULL,\n    price DECIMAL(10, 2) NOT NULL,\n    CONSTRAINT CK_Item_Price CHECK (price >= 0)\n  );\n\n-- Create the 'VendorItem' table ## we will assume all items from same vendor vendorDC rules\nCREATE TABLE IF NOT EXISTS VendorItem (\n    Item_id INTEGER NOT NULL,\n    Vendor_id INTEGER NOT NULL,\n    CONSTRAINT FK_VendorItem_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_VendorItem_Vendor FOREIGN KEY (Vendor_id) REFERENCES Vendor(Vendor_id),\n    CONSTRAINT UQ_VendorItem UNIQUE (Item_id, Vendor_id)\n  );\n\n-- Create the 'StoreDC' table\nCREATE TABLE IF NOT EXISTS StoreDC (\n    Store_id INTEGER NOT NULL,\n    DC_id INTEGER NOT NULL,\n    DC_StoreLT INTEGER NOT NULL,\n    CONSTRAINT FK_StoreDC_Store FOREIGN KEY (Store_id) REFERENCES Store(Store_id),\n    CONSTRAINT FK_StoreDC_DC_SDC FOREIGN KEY (DC_id) REFERENCES DC(DC_id),\n    CONSTRAINT UQ_StoreDC UNIQUE (Store_id, DC_id)\n  );\n\n-- Create the 'StoreFC' table\nCREATE TABLE IF NOT EXISTS StoreFC (\n    Store_id INTEGER NOT NULL,\n    Item_id INTEGER NOT NULL,\n    Forecast_Create_dt DATE NOT NULL,\n    Forecast_sales_dt DATE NOT NULL,\n    Forecast INTEGER NOT NULL,\n    CONSTRAINT FK_StoreFC_Store FOREIGN KEY (Store_id) REFERENCES Store(Store_id),\n    CONSTRAINT FK_StoreFC_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT UQ_StoreFC UNIQUE (Store_id, Item_id, Forecast_Create_dt, Forecast_sales_dt),\n    CONSTRAINT CK_StoreFC_Forecast CHECK (Forecast >= 0)\n  );\n\n-- Create the 'StoreSales' table\nCREATE TABLE IF NOT EXISTS StoreSales (\n    Store_id INTEGER NOT NULL,\n    Item_id INTEGER NOT NULL,\n    Sales_dt DATE NOT NULL,\n    Sales INTEGER NOT NULL,\n    CONSTRAINT FK_StoreSales_Store FOREIGN KEY (Store_id) REFERENCES Store(Store_id),\n    CONSTRAINT FK_StoreSales_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT UQ_StoreSales UNIQUE (Store_id, Item_id, Sales_dt),\n    CONSTRAINT CK_StoreSales_Sales CHECK (Sales >= 0)\n  );\n\n-- Create the 'InventoryOH' table # We use stckless DCs inventory is stored only in Store\nCREATE TABLE IF NOT EXISTS InventoryOH (\n    Item_id INTEGER NOT NULL,\n    Store_id INTEGER NOT NULL, \n    InventoryDt DATE NOT NULL,\n    BODQty INTEGER NOT NULL,\n    EODQty INTEGER NOT NULL,\n    CONSTRAINT FK_InventoryOH_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_InventoryOH_Store FOREIGN KEY (Store_id) REFERENCES Store(Store_id),\n    CONSTRAINT UQ_InventoryOH UNIQUE (Item_id, Store_id, InventoryDt),\n    CONSTRAINT CK_InventoryOH_BODQty CHECK (BODQty >= 0),\n    CONSTRAINT CK_InventoryOH_EODQty CHECK (EODQty >= 0)\n  );\n\n-- Create the 'orders' table\nCREATE TABLE IF NOT EXISTS orders (\n    order_id INTEGER PRIMARY KEY AUTOINCREMENT,\n    OrderDt DATE NOT NULL,\n    Item_id INTEGER NOT NULL,\n    Vendor_id INTEGER NOT NULL,\n    Store_id INTEGER NOT NULL,\n    OrderQty INTEGER NOT NULL,\n    CONSTRAINT FK_orders_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_orders_Vendor FOREIGN KEY (Vendor_id) REFERENCES Vendor(Vendor_id),\n    CONSTRAINT FK_orders_Store FOREIGN KEY (Store_id) REFERENCES Store(Store_id),\n    CONSTRAINT CK_orders_OrderQty CHECK (OrderQty > 0)\n  );\n--  order_dt can be pulled using order_id\n\n-- Create the 'VendorShips' table\nCREATE TABLE IF NOT EXISTS VendorShips (\n    Item_id INTEGER NOT NULL,\n    Vendor_id INTEGER NOT NULL,\n    order_id INTEGER NOT NULL, \n    ShipDt DATE NOT NULL,\n    ShipQty INTEGER NOT NULL,\n    CONSTRAINT FK_VendorShips_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_VendorShips_Vendor FOREIGN KEY (Vendor_id) REFERENCES Vendor(Vendor_id),\n    CONSTRAINT FK_VendorShips_orders FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    CONSTRAINT UQ_VendorShips_Order UNIQUE (Vendor_id, order_id, Item_id),\n    CONSTRAINT CK_VendorShips_ShipQty CHECK (ShipQty > 0)\n  );\n\n--  order_dt and store_id can can be pulled using order_id\n\n-- Create the 'DCShips' table\nCREATE TABLE IF NOT EXISTS DCShips (\n    Item_id INTEGER NOT NULL,\n    DC_id INTEGER NOT NULL, \n    order_id INTEGER NOT NULL, \n    ShipDt DATE NOT NULL,\n    ShipQty INTEGER NOT NULL,\n    CONSTRAINT FK_DCShips_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_DCShips_DC FOREIGN KEY (DC_id) REFERENCES DC(DC_id),\n    CONSTRAINT FK_DCShips_orders FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    CONSTRAINT UQ_DCShips_Order UNIQUE (DC_id, order_id, Item_id),\n    CONSTRAINT CK_DCShips_ShipQty CHECK (ShipQty > 0)\n  );\n\n-- Create the 'DCReceipts' table\n-- AVDCLT - actual vendor DC LT - we will simulate it and \n-- estimate ReceivedDt based on it - will not save it \n-- protect data integrity\n-- need to calculate AVDCLT as ReceivedDt - Orderdate\nCREATE TABLE IF NOT EXISTS DCReceipts (\n    order_id INTEGER NOT NULL, \n    Item_id INTEGER NOT NULL,\n    DC_id INTEGER NOT NULL, \n    ReceivedDt DATE NOT NULL,\n    ReceivedQty INTEGER NOT NULL,\n    CONSTRAINT FK_DCReceipts_orders FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    CONSTRAINT FK_DCReceipts_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_DCReceipts_DC FOREIGN KEY (DC_id) REFERENCES DC(DC_id),\n    CONSTRAINT UQ_DCReceipts_Order UNIQUE (order_id, Item_id, DC_id),\n    CONSTRAINT CK_DCReceipts_ReceivedQty CHECK (ReceivedQty >= 0)\n  );\n\n-- Create the 'StoreReceipts' table\nCREATE TABLE IF NOT EXISTS StoreReceipts (\n    order_id INTEGER NOT NULL, \n    Item_id INTEGER NOT NULL,\n    Store_id INTEGER NOT NULL, \n    ReceivedDt DATE NOT NULL,\n    ReceivedQty INTEGER NOT NULL,\n    CONSTRAINT FK_StoreReceipts_orders FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    CONSTRAINT FK_StoreReceipts_Item FOREIGN KEY (Item_id) REFERENCES Item(Item_id),\n    CONSTRAINT FK_StoreReceipts_Store FOREIGN KEY (Store_id) REFERENCES Store(Store_id),\n    CONSTRAINT UQ_StoreReceipts_Order UNIQUE (order_id, Item_id, Store_id),\n    CONSTRAINT CK_StoreReceipts_ReceivedQty CHECK (ReceivedQty >= 0)\n  );","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:21.964556Z","iopub.execute_input":"2025-04-20T04:32:21.96483Z","iopub.status.idle":"2025-04-20T04:32:22.172014Z","shell.execute_reply.started":"2025-04-20T04:32:21.96481Z","shell.execute_reply":"2025-04-20T04:32:22.171141Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Get constrains in the current db","metadata":{}},{"cell_type":"code","source":"import sqlite3\nimport re\n\ndef get_sqlite_constraints(db_file):\n    \"\"\"\n    Retrieves constraint information from an SQLite database.\n\n    Args:\n        db_file (str): The path to the SQLite database file.\n\n    Returns:\n        dict: A dictionary where keys are table names, and values are lists of\n              dictionaries, each representing a constraint.\n    \"\"\"\n    constraints = {}\n    try:\n        conn = sqlite3.connect(db_file)\n        cursor = conn.cursor()\n\n        # Get table names\n        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n        tables = [table[0] for table in cursor.fetchall()]\n\n        for table in tables:\n            constraints[table] = []\n            cursor.execute(f\"PRAGMA table_info('{table}');\")\n            columns = cursor.fetchall()\n\n            # Get table creation SQL\n            cursor.execute(f\"SELECT sql FROM sqlite_master WHERE type='table' AND name='{table}';\")\n            create_table_sql = cursor.fetchone()[0]\n\n            # Extract constraints from CREATE TABLE SQL\n            if create_table_sql:\n                # Primary Key\n                pk_match = re.search(r\"PRIMARY KEY \\((.*?)\\)\", create_table_sql, re.IGNORECASE)\n                if pk_match:\n                    pk_columns = [col.strip() for col in pk_match.group(1).split(\",\")]\n                    constraints[table].append({\n                        \"type\": \"PRIMARY KEY\",\n                        \"columns\": pk_columns\n                    })\n\n                # Foreign Keys\n                fk_matches = re.finditer(r\"FOREIGN KEY \\((.*?)\\) REFERENCES (.*?)(\\((.*?)\\))?\", create_table_sql, re.IGNORECASE)\n                for fk_match in fk_matches:\n                    fk_columns = [col.strip() for col in fk_match.group(1).split(\",\")]\n                    ref_table = fk_match.group(2).strip()\n                    ref_columns = [col.strip() for col in fk_match.group(4).split(\",\")] if fk_match.group(4) else None\n                    constraints[table].append({\n                        \"type\": \"FOREIGN KEY\",\n                        \"columns\": fk_columns,\n                        \"referenced_table\": ref_table,\n                        \"referenced_columns\": ref_columns\n                    })\n\n                # Unique Constraints\n                unique_matches = re.finditer(r\"UNIQUE \\((.*?)\\)\", create_table_sql, re.IGNORECASE)\n                for unique_match in unique_matches:\n                    unique_columns = [col.strip() for col in unique_match.group(1).split(\",\")]\n                    constraints[table].append({\n                        \"type\": \"UNIQUE\",\n                        \"columns\": unique_columns\n                    })\n\n                # Check Constraints\n                check_matches = re.finditer(r\"CHECK \\((.*?)\\)\", create_table_sql, re.IGNORECASE)\n                for check_match in check_matches:\n                    check_condition = check_match.group(1).strip()\n                    constraints[table].append({\n                        \"type\": \"CHECK\",\n                        \"condition\": check_condition\n                    })\n\n            # NOT NULL constraints (from table_info)\n            for column in columns:\n                if column[2] == 0:  # 0 indicates NOT NULL\n                    constraints[table].append({\n                        \"type\": \"NOT NULL\",\n                        \"column\": column[1]\n                    })\n\n    except sqlite3.Error as e:\n        print(f\"SQLite error: {e}\")\n    finally:\n        if conn:\n            conn.close()\n    return constraints\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:22.175549Z","iopub.execute_input":"2025-04-20T04:32:22.17601Z","iopub.status.idle":"2025-04-20T04:32:22.187449Z","shell.execute_reply.started":"2025-04-20T04:32:22.175989Z","shell.execute_reply":"2025-04-20T04:32:22.186363Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example Usage\ndb_file = \"SupplyChainABC2.db\"\nconstraints = get_sqlite_constraints(db_file)\n\nfor table, table_constraints in constraints.items():\n    print(f\"--- Table: {table} ---\")\n    for constraint in table_constraints:\n        print(f\"  - Type: {constraint['type']}\")\n        if constraint['type'] == 'PRIMARY KEY' or constraint['type'] == 'UNIQUE' or constraint['type'] == 'FOREIGN KEY':\n            print(f\"    Columns: {constraint.get('columns')}\")\n            if constraint['type'] == 'FOREIGN KEY':\n                print(f\"    Referenced Table: {constraint.get('referenced_table')}\")\n                print(f\"    Referenced Columns: {constraint.get('referenced_columns')}\")\n        elif constraint['type'] == 'CHECK':\n            print(f\"    Condition: {constraint.get('condition')}\")\n        elif constraint['type'] == 'NOT NULL':\n            print(f\"    Column: {constraint.get('column')}\")\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:22.188413Z","iopub.execute_input":"2025-04-20T04:32:22.18868Z","iopub.status.idle":"2025-04-20T04:32:22.213314Z","shell.execute_reply.started":"2025-04-20T04:32:22.188651Z","shell.execute_reply":"2025-04-20T04:32:22.212532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # To run tests not needed now\n# %%sql\n# -- To delete and recreate a table\n# -- Create the 'DC' table\n# DROP TABLE IF EXISTS Vendor;\n# -- Create the 'Vendor' table\n# CREATE TABLE IF NOT EXISTS Vendor (\n#   \tVendor_id INTEGER PRIMARY KEY AUTOINCREMENT,\n#   \tVendor_name VARCHAR(255) NOT NULL,\n#   \tVendorCity VARCHAR(255) NOT NULL\n#   );\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.214242Z","iopub.execute_input":"2025-04-20T04:32:22.214522Z","iopub.status.idle":"2025-04-20T04:32:22.223891Z","shell.execute_reply.started":"2025-04-20T04:32:22.214504Z","shell.execute_reply":"2025-04-20T04:32:22.22303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step-3 : Create Tables That Define Digital Twin Network.","metadata":{}},{"cell_type":"code","source":"# 1. DC\n# Get 5 random cities from the top 50\nimport random\n\nmax_DC_Number = 2\ntop_50_us_cities = [\n \"New York, NY\", \"Los Angeles, CA\", \"Chicago, IL\", \n \"Houston, TX\", \"Phoenix, AZ\", \"Philadelphia, PA\",\n \"San Antonio, TX\", \"San Diego, CA\", \"Dallas, TX\",\n \"San Jose, CA\", \"Austin, TX\", \"Jacksonville, FL\",\n \"Fort Worth, TX\", \"Columbus, OH\", \"Charlotte, NC\",\n \"San Francisco, CA\", \"Indianapolis, IN\", \"Seattle, WA\",\n \"Denver, CO\", \"Washington, DC\", \"Boston, MA\",\n \"El Paso, TX\", \"Nashville, TN\", \"Detroit, MI\",\n \"Oklahoma City, OK\", \"Portland, OR\", \"Las Vegas, NV\",\n \"Memphis, TN\", \"Louisville, KY\", \"Baltimore, MD\",\n \"Milwaukee, WI\", \"Albuquerque, NM\", \"Tucson, AZ\",\n \"Fresno, CA\", \"Sacramento, CA\", \"Kansas City, MO\",\n \"Mesa, AZ\", \"Atlanta, GA\", \"Omaha, NE\",\n \"Colorado Springs, CO\", \"Raleigh, NC\", \"Miami, FL\",\n \"Virginia Beach, VA\", \"Oakland, CA\", \"Minneapolis, MN\",\n \"Tulsa, OK\", \"Arlington, TX\", \"New Orleans, LA\",\n \"Wichita, KS\", \"Cleveland, OH\"\n]\n\n# random_cities = random.sample(top_50_us_cities, 2)\n\n# print(random_cities)\n# DCCity = [top_50_us_cities[n] for n in range(max_DC_Number)]\n\nDCCity = [top_50_us_cities[n] for n in range(max_DC_Number)]\nDC_name = [\"DC_\" + DCCity[n] for n in range(max_DC_Number)]\n#print(DCCity,DC_name)\nDC = pd.DataFrame({\n    \"DC_name\": DC_name,\n    \"DCCity\": DCCity})\nDC\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'DC'\ninsert_dataframe_to_sqlite(DC, db_name, table_name)\nexecute_query(\"select * from DC\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.224793Z","iopub.execute_input":"2025-04-20T04:32:22.225043Z","iopub.status.idle":"2025-04-20T04:32:22.263805Z","shell.execute_reply.started":"2025-04-20T04:32:22.225023Z","shell.execute_reply":"2025-04-20T04:32:22.263101Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"execute_query(\"select * from DC\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:22.264582Z","iopub.execute_input":"2025-04-20T04:32:22.264782Z","iopub.status.idle":"2025-04-20T04:32:22.270684Z","shell.execute_reply.started":"2025-04-20T04:32:22.264766Z","shell.execute_reply":"2025-04-20T04:32:22.26998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"execute_query(\"select * from DC\",debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:22.271639Z","iopub.execute_input":"2025-04-20T04:32:22.271925Z","iopub.status.idle":"2025-04-20T04:32:22.289528Z","shell.execute_reply.started":"2025-04-20T04:32:22.271897Z","shell.execute_reply":"2025-04-20T04:32:22.288635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Store\nnumrecs = 30\nStoreCity = [top_50_us_cities[n % 50] for n in range(2, 2+numrecs)]\nStore_name = [\"Store_\" + StoreCity[n] for n in range(numrecs)]\n# print(DCCity,DC_name)\nStore = pd.DataFrame({\n    \"Store_name\": Store_name,\n    \"StoreCity\": StoreCity})\n# DC\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'Store'\ninsert_dataframe_to_sqlite(Store, db_name, table_name)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.290615Z","iopub.execute_input":"2025-04-20T04:32:22.290918Z","iopub.status.idle":"2025-04-20T04:32:22.311856Z","shell.execute_reply.started":"2025-04-20T04:32:22.290893Z","shell.execute_reply":"2025-04-20T04:32:22.311174Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"execute_query(\"select * from Store\")[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:22.312767Z","iopub.execute_input":"2025-04-20T04:32:22.312996Z","iopub.status.idle":"2025-04-20T04:32:22.319506Z","shell.execute_reply.started":"2025-04-20T04:32:22.312978Z","shell.execute_reply":"2025-04-20T04:32:22.31878Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Vendor\nimport random\nnumrecs = 10\nrandom_cities = random.sample(top_50_us_cities, numrecs)\n\nVendorCity = random_cities\nVendor_name = [\"Vendor_\" + VendorCity[n] for n in range(numrecs)]\n# print(DCCity,DC_name)\ndata = pd.DataFrame({\n    \"Vendor_name\": Vendor_name,\n    \"VendorCity\": VendorCity})\n# DC\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'Vendor'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.320584Z","iopub.execute_input":"2025-04-20T04:32:22.320812Z","iopub.status.idle":"2025-04-20T04:32:22.348552Z","shell.execute_reply.started":"2025-04-20T04:32:22.320794Z","shell.execute_reply":"2025-04-20T04:32:22.347563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. VendorDC\nimport random\nimport pandas as pd\n\nv = 10\nd = 2\nVendor_id = [n+1 for n in range(v)]\nDC_id = [n+1 for n in range(d)]\n# Initially I thought vendor goes to only one of the 2 DCs\n# but we have all items in all stores so vendor should go to all DCs n%2+1 should get 1 or 2 alternatively\n\nVendor_DC_LT = [random.randint(5, 10) for n in range(numrecs)]\n\nvdclt = list((Vendor_id[x],DC_id[y],max(5,[np.random.normal(7, 1, 1).astype(int)][0][0])) \n             for x in range(v) for y in range(d))\n\n\ndata = pd.DataFrame({\n    \"Vendor_id\":[col[0] for col in vdclt],\n    \"DC_id\": [col[1] for col in vdclt],\n    \"Vendor_DC_LT\": [col[2] for col in vdclt]})\n\n\ndata\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'VendorDC'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:5]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.34954Z","iopub.execute_input":"2025-04-20T04:32:22.350158Z","iopub.status.idle":"2025-04-20T04:32:22.370258Z","shell.execute_reply.started":"2025-04-20T04:32:22.350122Z","shell.execute_reply":"2025-04-20T04:32:22.369631Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Item\nnumrecs = 20\ngrocery_items = [\n \"Eggs (Large, Grade A)\",\n \"Eggs (Organic, Free-Range)\",\n \"Milk (Whole, 1 Gallon)\",\n \"Bread (Whole Wheat)\",\n \"Bananas (per lb)\",\n \"Chicken Breast (Boneless, Skinless)\",\n \"Ground Beef (80% Lean)\",\n \"Rice (White, Long Grain)\",\n \"Pasta (Spaghetti)\",\n \"Cereal (Corn Flakes)\",\n \"Apples (Red Delicious, per lb)\",\n \"Potatoes (Russet, 5 lb bag)\",\n \"Tomatoes (Vine-Ripened, per lb)\",\n \"Cheese (Cheddar, 8 oz block)\",\n \"Yogurt (Plain, Greek)\",\n \"Butter (Salted, 1 lb)\",\n \"Orange Juice (100% Pure, 64 oz)\",\n \"Coffee (Ground, Medium Roast)\",\n \"Canned Tuna (in Water, 5 oz)\",\n \"Frozen Mixed Vegetables (16 oz bag)\"\n]\nimport random\nItem_name = [grocery_items[n] for n in range(numrecs)]\n\nprice = [random.randint(5, 20)+random.randint(5, 9)/10+random.randint(5, 9)/100 for n in range(numrecs)]\n# print(DCCity,DC_name)\ndata = pd.DataFrame({\n    \"Item_name\": Item_name,\n    \"price\": price})\ndata\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'Item'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:5]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.371231Z","iopub.execute_input":"2025-04-20T04:32:22.371538Z","iopub.status.idle":"2025-04-20T04:32:22.389258Z","shell.execute_reply.started":"2025-04-20T04:32:22.371519Z","shell.execute_reply":"2025-04-20T04:32:22.38835Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. VendorItem \nimport random\nimport pandas as pd\nnumrecs = 20\nItem_id = [n+1 for n in range(numrecs)]\nVendor_id = [int(n/2)+1 for n in range(numrecs)]# should get 1 to 10\n# each vendor supplies 2 items\n\n# print(DCCity,DC_name)\ndata = pd.DataFrame({\n    \"Item_id\": Item_id,\n    \"Vendor_id\":Vendor_id})\ndata\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'VendorItem'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:10]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.390228Z","iopub.execute_input":"2025-04-20T04:32:22.390565Z","iopub.status.idle":"2025-04-20T04:32:22.406747Z","shell.execute_reply.started":"2025-04-20T04:32:22.390539Z","shell.execute_reply":"2025-04-20T04:32:22.406078Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. StoreDC\nimport random\nimport pandas as pd\nnumrecs = 30\nStore_id = [n+1 for n in range(numrecs)]\nDC_id = [n%2+1 for n in range(numrecs)]# should get 1 or 2 alternatively\nDC_StoreLT = [random.randint(1, 3) for n in range(numrecs)]\n\n# print(DCCity,DC_name)\ndata = pd.DataFrame({\n    \"Store_id\":Store_id,\n    \"DC_id\": DC_id,\n    \"DC_StoreLT\": DC_StoreLT})\ndata\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'StoreDC'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:10]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.407652Z","iopub.execute_input":"2025-04-20T04:32:22.407881Z","iopub.status.idle":"2025-04-20T04:32:22.426801Z","shell.execute_reply.started":"2025-04-20T04:32:22.407864Z","shell.execute_reply":"2025-04-20T04:32:22.426048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step-4 : Create Initial Forecast and sales Data.","metadata":{}},{"cell_type":"code","source":"# To set Planning Period Globally\ndaysinplan = 10 # for how many days we are running ERP Process and generating data\n# FCdt = '2025-01-01' # Not changed but want to change in the future\nplanStartdt = '2025-01-01'\n\n%pdb on\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.427706Z","iopub.execute_input":"2025-04-20T04:32:22.428012Z","iopub.status.idle":"2025-04-20T04:32:22.434145Z","shell.execute_reply.started":"2025-04-20T04:32:22.427982Z","shell.execute_reply":"2025-04-20T04:32:22.43326Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Table 8. StoreFC","metadata":{}},{"cell_type":"code","source":"# 8. StoreFC - This creates It Store Forecasts on Day1 for all the Days in our Plan 30 days or saet below\n# Any changes should match with the ERP process cell all the way at the end\nimport datetime\nimport random\n\n# daysinplan = 30 # for how many days we are running ERP Process and generating data\n\nnp.random.seed(42)\n# dates = pd.date_range(start=\"2025-01-01\", periods=30, freq=\"D\")\n# dates = pd.to_datetime(pd.date_range(start=\"2025-01-01\", periods=daysinplan, freq=\"D\"))\n# day1 is already processed so start from day 2 and run n-1 days\n# dates = pd.to_datetime(pd.date_range(start=\"2025-01-01\", periods=daysinplan, freq=\"D\")).strftime(\"%Y-%m-%d\")\n# dates = pd.date_range(start=\"2025-01-01\", periods=daysinplan, freq=\"D\").strftime(\"%Y-%m-%d\")\ndates = pd.date_range(start=planStartdt, periods=daysinplan, freq=\"D\").strftime(\"%Y-%m-%d\")\n\n\nst = 30\nit = 20\n\n\nStore_id = [n+1 for n in range(st)]\nItem_id = [n+1 for n in range(it)]# should get 1 to 20 for each store\n\nItMeanFC = np.random.normal(30, 4, it).astype(int) #Each item will have a mean FC(30),stdev(4) set here for it(20) items\nStStdevFC = np.random.normal(10, 2, st).astype(int) #Each item will have a stdev FC set here with mean 10, stdev 2 for the 30 stores\nitstfc = list((Store_id[x],Item_id[y],max(5,[np.random.normal(ItMeanFC[y], StStdevFC[x], 1).astype(int)][0][0])) for x in range(st) for y in range(it))\n\n# repeat the itstfcst for the Forecast_Create_dt (we limited it to just '2025-01-01')\n# FCdt = '2025-01-01'\nFCdt = planStartdt\n# and Forecast_sales_dt - 30days starting from '2025-01-01'\n\ndata = pd.DataFrame({\n    \"Store_id\":[col[0] for col in itstfc],\n    \"item_id\": [col[1] for col in itstfc],\n    \"Forecast\": [col[2] for col in itstfc]})\n\ndata[\"Forecast_Create_dt\"]=FCdt\ndata[\"Forecast_sales_dt\"]=dates[0] # first fcst sales date is also same\n# print(\"starting data combined\")\ndatacombined = data.copy(deep=True) # if it is just assignment this will get modified when data\n# get modified\n\n# print(datacombined)\n\nfor i in range(len(dates)-1): #len(dates)-1\n    data[\"Forecast_sales_dt\"]=dates[i+1]\n    # print(i, \" data in loop\")\n    # print(data)\n    # print(i, \" datacombined in loop\")\n    # print(datacombined)\n    datacombined = pd.concat([datacombined, data], axis=0)\n\nnewcolorder = ['Store_id','item_id','Forecast_Create_dt','Forecast_sales_dt','Forecast']\ndatacombined = datacombined[newcolorder]\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'StoreFC'\ninsert_dataframe_to_sqlite(datacombined, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:10]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.435071Z","iopub.execute_input":"2025-04-20T04:32:22.435383Z","iopub.status.idle":"2025-04-20T04:32:22.520665Z","shell.execute_reply.started":"2025-04-20T04:32:22.435356Z","shell.execute_reply":"2025-04-20T04:32:22.519975Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Table 9. StoreSales - for day 1","metadata":{}},{"cell_type":"code","source":"# 9. StoreSales - for day 1 - uses datacombined created from 8. StoreFC\n# This is only for the first day of sales, where we assume enough inventory.  \n# Rest, is run as part of the ERP process as sales has to be\n# limited by inventory availability\n\nimport datetime\nimport random\n\nsoldout_rate = 0.05\n# we will generate store sales by assuming the itstfc as the mean, project a normal \n# distribution with an assumed stdev\n\nnp.random.seed(42)\n# datacombined['Forecast'].max()\n\nsalesdata = datacombined[['Store_id','item_id','Forecast_sales_dt','Forecast']].copy(deep=True) \nplanStartdt\n# salesdata = salesdata[salesdata['Forecast_sales_dt']=='2025-01-01']\nsalesdata = salesdata[salesdata['Forecast_sales_dt']== planStartdt]\n# if it is just assignment this will get modified when data\nsalesdata['Sales_dt'] = salesdata['Forecast_sales_dt']\nsalesdata['Sales']= [max(2,[np.random.normal(x, 10, 1).astype(int)][0][0]) for x in salesdata['Forecast']]\n\ndata = salesdata[['Store_id','item_id','Sales_dt','Sales']]\n# data\nfrstDaySale = data\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'StoreSales'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:10]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.52151Z","iopub.execute_input":"2025-04-20T04:32:22.52173Z","iopub.status.idle":"2025-04-20T04:32:22.553199Z","shell.execute_reply.started":"2025-04-20T04:32:22.521712Z","shell.execute_reply":"2025-04-20T04:32:22.552438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# qry = \"select * from StoreSales\"\n# qry2 = \"Select count(*) from (\" + qry + \")\"\n# execute_query(qry2)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.553997Z","iopub.execute_input":"2025-04-20T04:32:22.554267Z","iopub.status.idle":"2025-04-20T04:32:22.558088Z","shell.execute_reply.started":"2025-04-20T04:32:22.554249Z","shell.execute_reply":"2025-04-20T04:32:22.557227Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step-5 : Define Data Retrival Function and Create Initial Inventory","metadata":{}},{"cell_type":"code","source":"# fn cn:sqlite3.Connection in fn below specifies that the input is of type cn:sqlite3.Connection.  The -> pd.DataFrame specifies that it must return a DataFrame\n# ItVdcST_LTFCSalesdata is used for Day1 Inventory ##########\ndef extract_ItVdcST_LTFCSalesdata(dt, cn:sqlite3.Connection, debug=False)-> pd.DataFrame:\n    import sqlite3\n    import pandas as pd\n    \n    # Establish a connection to the SQLite database\n    # connection = sqlite3.connect('your_database.db')\n    \n    # Write the SQL query to retrieve the data\n\n    query1  = \"\"\"\n    \n    SELECT \n        I.Item_id,\n        V.Vendor_id,\n    \tV.DC_id,\n        S.Store_id,\n        V.Vendor_DC_LT, \n        S.DC_StoreLT,\n     \tVendor_DC_LT + DC_StoreLT AS SVSLT,\n        SF.InventoryDt,\n        SF.Forecast,\n        SS.Sales_dt, \n        SS.Sales\n    FROM \n    VendorDC V,\n    StoreDC S,\n    VendorItem I,\n    (Select Item_id,Store_id, Forecast, Forecast_sales_dt as InventoryDt  \n        FROM StoreFC where Forecast_sales_dt = \"\"\"\n   \n    query2 = \"\"\" \n     ) SF,\n     (Select Store_id, Item_id, Sales_dt, Sales from StoreSales where Sales_dt = \"\"\"\n    \n    query3 = \"\"\" ) SS\n    where \n    V.DC_id = S.DC_id and\n    V.Vendor_id = I.Vendor_id and \n    S.Store_id = SF.Store_id and \n    I.Item_id = SF.Item_id and\n    S.Store_id = SS.Store_id and \n    I.Item_id = SS.Item_id\n    \n    --GROUP BY 1,2,3\n    ORDER BY 1,2,3,4 ;\n    \n    \"\"\"\n    query = query1 + \"'\" + dt + \"'\" + query2 + \"'\" + dt + \"'\" + query3\n    if debug:\n        print(query)\n    # Use pandas to read the data into a DataFrame\n    # df1 = pd.read_sql_query(query1, db_conn)\n    dfItVdcST_LTFCSalesdata = pd.read_sql_query(query, cn)\n    # Close the connection\n    # connection.close()\n    \n    # Now, 'df' contains the data from the SQLite table\n    return(dfItVdcST_LTFCSalesdata)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.558928Z","iopub.execute_input":"2025-04-20T04:32:22.559211Z","iopub.status.idle":"2025-04-20T04:32:22.574487Z","shell.execute_reply.started":"2025-04-20T04:32:22.559187Z","shell.execute_reply":"2025-04-20T04:32:22.573689Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Create Table 10. Initial (Day1) InventoryOH","metadata":{}},{"cell_type":"code","source":"# 10. Initial (Day1) InventoryOH  uses ItVdcST_LTFCSalesdata from above\n# uses sales to calculate correct EOD\nimport datetime\nimport random\n\n# dt = '2025-01-01'\ndt = planStartdt\nItVdcST_LTFCSalesdata = []\nItVdcST_LTFCSalesdata=extract_ItVdcST_LTFCSalesdata(dt, db_conn,debug=True)\n\n\n# we will generate InventoryOH on 1st day as OUTL = D(LT+RT)+SS\n# SS we will assume 1 DOS and review every day so OUTL = D(LT+1+1)\n# distribution with an assumed stdev\n\nnp.random.seed(42)\n# datacombined['Forecast'].max()\n\nItVdcST_LTFCSalesdata['BODQty'] = (ItVdcST_LTFCSalesdata['SVSLT']+2) * ItVdcST_LTFCSalesdata['Forecast']\n# for now we assume EODQty is same as BODQty.  Later we will run sales which will reduce the EODQty by Sales\n\n# merged_df = pd.merge(frstDaySale,ItVdcST ,on=['Store_id', 'Item_id'])\n# # merged_df = pd.merge(df1, df2, on=['key1', 'key2'])\n#  merge didn't work\n\nItVdcST_LTFCSalesdata['EODQty'] = ItVdcST_LTFCSalesdata['BODQty'] - ItVdcST_LTFCSalesdata['Sales']\n# - merged_df['Sales']\n# and run replenishment which will order new units\n# ItVdcST\n\ndata = []\n\ndata = ItVdcST_LTFCSalesdata[['Item_id','Store_id','InventoryDt','BODQty','EODQty']]\n# data\n\n# insert \ndb_name = 'SupplyChainABC2.db'\ntable_name = 'InventoryOH'\ninsert_dataframe_to_sqlite(data, db_name, table_name)\nexecute_query(\"select * from \" + table_name)[:10]","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.575398Z","iopub.execute_input":"2025-04-20T04:32:22.57564Z","iopub.status.idle":"2025-04-20T04:32:22.620414Z","shell.execute_reply.started":"2025-04-20T04:32:22.575622Z","shell.execute_reply":"2025-04-20T04:32:22.619561Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"table_name = 'InventoryOH'\n\nexecute_query(\"select * from \" + table_name)[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:22.621359Z","iopub.execute_input":"2025-04-20T04:32:22.621592Z","iopub.status.idle":"2025-04-20T04:32:22.629125Z","shell.execute_reply.started":"2025-04-20T04:32:22.621575Z","shell.execute_reply":"2025-04-20T04:32:22.628257Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Step 6 - Define Other Helper Functions and Create the ERP Process ","metadata":{}},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"markdown","source":"**6a. Extract Data and Record Sales**","metadata":{}},{"cell_type":"code","source":"# This is used to extract data for sales calculation on nth day except first\n# By this time we have FCST for all days and Inventory (EODQty) for previous day\n\n# fn cn:sqlite3.Connection in fn below specifies that the input is of type cn:sqlite3.Connection.  The -> pd.DataFrame specifies that it must return a DataFrame\ndef extract_ItSTDminus1EODasBODdnFC(sdt,ohdt, cn:sqlite3.Connection, debug = False)-> pd.DataFrame:\n    import sqlite3\n    import pandas as pd\n    \n    # Establish a connection to the SQLite database\n    # connection = sqlite3.connect('your_database.db')\n    \n    # Write the SQL query to retrieve the data\n\n    query1  = \"\"\"\n\n    SELECT \n        SF.Item_id,\n        SF.Store_id,\n        OH.InventoryDt,\n        SF.Forecast_sales_dt,\n        OH.Dminus1EODasBODQtyn,\n        SF.Forecast\n    FROM \n    (Select F.Item_id,F.Store_id, F.Forecast_sales_dt, F.Forecast   \n        FROM \n        StoreFC F\n        where \n        F.Forecast_sales_dt = \"\"\"\n   \n    query2 = \"\"\" \n     ) SF,\n     (Select Item_id,Store_id, InventoryDt, EODQty as Dminus1EODasBODQtyn  \n        FROM InventoryOH where InventoryDt = \"\"\"\n   \n    query3 = \"\"\" \n     ) OH\n    where \n    SF.Item_id = OH.Item_id and\n    SF.Store_id = OH.Store_id \n    --GROUP BY 1,2,3\n    ORDER BY 1,2 ;\n    \n    \"\"\"\n    query = query1 + \"'\" + sdt + \"'\" + query2 + \"'\" + ohdt + \"'\" + query3\n    if debug:\n        print(query)\n    # Use pandas to read the data into a DataFrame\n    # df1 = pd.read_sql_query(query1, db_conn)\n    dfItSTDminus1EODasBODdnFC = pd.read_sql_query(query, cn)\n    # Close the connection\n    # connection.close()\n    \n    # Now, 'df' contains the data from the SQLite table\n    return(dfItSTDminus1EODasBODdnFC)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.629961Z","iopub.execute_input":"2025-04-20T04:32:22.630195Z","iopub.status.idle":"2025-04-20T04:32:22.640859Z","shell.execute_reply.started":"2025-04-20T04:32:22.630177Z","shell.execute_reply":"2025-04-20T04:32:22.639998Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Define Daily Process Functions - 1. Recordsales\n#soldout_rate = 0.05\n#soldout_df = \ndef Recordsales(dt,ItSTDminus1EODasBODdnFC,soldout_rate = 0.05, debug=False):\n    # We set Demand as a random function of forecast\n    # Sales is demand or boday (Beginning of Day) inventory, whichever is smaller.  We can only sell what we have\n    # Record this for all items and all stores\n    # For the day dt\n    \n    # we will generate store sales by assuming the itstfc as the mean, project a normal distribution with an assumed stdev\n    import random\n    np.random.seed(42)\n    # datacombined['Forecast'].max()\n    sdata = pd.DataFrame([])\n    ItSTDminus1EODasBODdnFC = ItSTDminus1EODasBODdnFC.rename(columns={'Forecast_sales_dt':'Sales_dt'})\n    sdata = ItSTDminus1EODasBODdnFC[['Store_id','Item_id','InventoryDt','Sales_dt','Dminus1EODasBODQtyn','Forecast']]\n    # if it is just assignment this will get modified as linked data\n    # Forecast_sales_dt is renamed as InventoryDt in the query as it was first wrote for that table\n    print(type(sdata))\n    # sdata = sdata.rename(columns={'Forecast_sales_dt':'Sales_dt'})\n    \n    # df = df.rename(columns={'A': 'Alpha'}) #print(df)\n    \n    # for each it, st combination, generate 1 sales value with mean = Forecast, stdev = 10 but keep a \n    #  minimum value of 2 - remember max (2,-3) = 2\n    salesrows = len(ItSTDminus1EODasBODdnFC)\n    \n    # create a normal distribution of multiplication factors with mean 1 and stdev 0.2\n    # to generate sales from fc\n    # dfm = pd.DataFrame(np.random.normal(0.5, 0.2, salesrows),columns=['Mfact'])\n    dfm = pd.DataFrame(np.random.normal(1, 0.2, salesrows),columns=['Mfact'])\n    # replace lt 0 (negative) values with 0\n    dfm.Mfact= dfm.Mfact.mask(dfm.Mfact.lt(0),0)\n    salesdata2 = []\n    # Concatenate to the sales data\n    salesdata2 = pd.concat([sdata, dfm], axis=1)\n    \n    \n    salesdata2['Sales']= (salesdata2['Forecast'] * salesdata2['Mfact']).round(0)\n    \n    # However, we can't sell more than what we have ininventory so set sales two min of fc, \n    # and sales calculated above\n    salesdata3 = []\n    salesdata3 = salesdata2\n\n    # Create a mask for 5% soldout_rate random selection\n    mask = np.random.rand(len(salesdata3)) < soldout_rate\n    salesdata3.loc[mask, 'Sales'] = salesdata3.loc[mask, 'Dminus1EODasBODQtyn']\n\n    salesdata3['Sales']= salesdata3[['Sales','Dminus1EODasBODQtyn']].apply(min,axis=1)\n    # record all sold outs\n    sold_out_df = salesdata3[salesdata3['Sales']==salesdata3['Dminus1EODasBODQtyn']].copy(deep=True)\n    sold_out_df = sold_out_df[['Store_id','Item_id','Sales_dt','Sales']]\n    \n    data = []\n    data = salesdata3[['Store_id','Item_id','Sales_dt','Sales']]\n    if debug:\n        print('data from record sales fn')\n        print(data)\n    db_name = 'SupplyChainABC2.db'\n    table_name = 'StoreSales'\n    insert_dataframe_to_sqlite(data, db_name, table_name)\n    execute_query(\"select * from \" + table_name)\n    return sold_out_df\n\n    \n\n    # Return() - return statement is not mandotory\n    \n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.641792Z","iopub.execute_input":"2025-04-20T04:32:22.642488Z","iopub.status.idle":"2025-04-20T04:32:22.66326Z","shell.execute_reply.started":"2025-04-20T04:32:22.64246Z","shell.execute_reply":"2025-04-20T04:32:22.662325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6b. Extract Data and Record Receipt of Ordered Inventory**","metadata":{}},{"cell_type":"code","source":"# This is to extract LT,Orders and recd Table data to record Receipts\n# fn cn:sqlite3.Connection in fn below specifies that the input is of type cn:sqlite3.Connection.  The -> pd.DataFrame specifies that it must return a DataFrame\n\n# I don't need dt for this as I am only looking at orders not yet received\ndef extract_ItVdcST_LTOrdRecD_data(cn:sqlite3.Connection,debug=False)-> pd.DataFrame:\n    import sqlite3\n    import pandas as pd\n    \n    # Establish a connection to the SQLite database\n    # connection = sqlite3.connect('your_database.db')\n    \n    # Write the SQL query to retrieve the data\n    # SVSLT - System Vendor store LT\n    \n    query1  = \"\"\"\n    Select \n        VSD.Item_id,\n        VSD.Vendor_id,\n    \tVSD.DC_id,\n        VSD.Store_id,\n        VSD.Vendor_DC_LT, \n        VSD.DC_StoreLT,\n     \tVSD.SVSLT,\n        OT.order_id,\n        OT.OrderDt,\n        OT.OrderQty\n\n    FROM\n    \n    (SELECT \n        I.Item_id,\n        V.Vendor_id,\n    \tV.DC_id,\n        S.Store_id,\n        V.Vendor_DC_LT, \n        S.DC_StoreLT,\n     \tVendor_DC_LT + DC_StoreLT AS SVSLT\n    FROM \n    VendorDC V,\n    StoreDC S,\n    VendorItem I\n    where \n    V.DC_id = S.DC_id and\n    V.Vendor_id = I.Vendor_id  \n    \n    ) AS VSD\n    \n    LEFT JOIN\n    \n    (SELECT \n        order_id,\n        OrderDt,\n        Item_id,\n        Vendor_id,\n        Store_id,\n        OrderQty\n    From\n        orders\n    where \n        order_id\n    NOT IN (\n            SELECT DISTINCT\n                order_id \n            FROM\n                StoreReceipts)\n                ) AS OT      \n    ON    \n  \tVSD.Item_id = OT.Item_id  and\n  \tVSD.Vendor_id = OT.Vendor_id  and\n    VSD.Store_id = OT.Store_id \n    ORDER BY \n    VSD.Item_id,\n    VSD.Vendor_id,\n    VSD.DC_id,\n    VSD.Store_id;\n\n    \n    \"\"\"\n    # query = query1 + \"'\" + dt + \"'\" + query2 + \"'\" + dt + \"'\" + query3\n    query = query1 \n    if debug:\n        print(query)\n    # Use pandas to read the data into a DataFrame\n    # df1 = pd.read_sql_query(query1, db_conn)\n    dfItVdcST_LTOrdRecD_data = pd.read_sql_query(query, cn)\n    # Close the connection\n    # connection.close()\n    \n    # Now, 'df' contains the data from the SQLite table\n    return(dfItVdcST_LTOrdRecD_data)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.664333Z","iopub.execute_input":"2025-04-20T04:32:22.664597Z","iopub.status.idle":"2025-04-20T04:32:22.684838Z","shell.execute_reply.started":"2025-04-20T04:32:22.664579Z","shell.execute_reply":"2025-04-20T04:32:22.683896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Define Daily ERP Process Functions - 2. RecordReceipts\ndef RecordReceipts(dt,ItVdcST_LTOrdRecD_data,debug=False):\n    \n    # We set Vendor to DC LT and DC to store LT as a random function of the system values in the tables\n    # Using the same normal distribution technique used in sales we use those values as mean and draw sample values \n    # from a normal distribution with mean = to the LT in system\n\n    import random\n    np.random.seed(42)\n    # datacombined['Forecast'].max()\n    odata = pd.DataFrame([])\n    \n    odata = ItVdcST_LTOrdRecD_data[['order_id','Item_id','Store_id','OrderDt','OrderQty','Vendor_DC_LT','DC_StoreLT','SVSLT']]\n    print(type(odata))\n\n    orows = len(ItVdcST_LTOrdRecD_data)\n    \n    # create a normal distribution of multiplication factors with mean 1 and stdev 0.15\n    # This simulates the actual LT (time in days it actually takes to deliver from the order date) as a value from a \n    # normal distribution with mean as system lead time and 15% of it as the stad deviation.\n    # system lead time is the lead time entered into the system based on discussion with\n    # the vendor and the distance btw vendor and dc.  It generally has 3 components \n    # 1. Vendor Processing Time(the estimated time it takes for the vendor to assemble the products ready for shipping)\n    # 2. Transportation time to the DC\n    # 3. DC Receiving time\n    # Similarly for the DC to store system LT\n    # In reality all three values are highly variable which is why we are simulating the actual values\n\n    \n    dfm = pd.DataFrame(np.random.normal(1, 0.15, orows),columns=['Mfact'])\n    # replace lt 0.5 (negative) values with 0.5 we are not expecting LT to be LT 50% of the actual\n    dfm.Mfact= dfm.Mfact.mask(dfm.Mfact.lt(0.5),0.5)\n    odata2 = []\n    # Concatenate to the sales data\n    odata2 = pd.concat([odata, dfm], axis=1)\n    \n    \n    odata2['Actual_V_DC_LT']= (odata2['Vendor_DC_LT'] * odata2['Mfact']).round(0)\n    odata2['Actual_DC_StoreLT']= (odata2['DC_StoreLT'] * odata2['Mfact']).round(0)\n    odata2['Actual_V_StoreLT']= odata2['Actual_V_DC_LT'] + odata2['Actual_DC_StoreLT']\n    # odata2['ReceivedDt']= datetime.strptime(odata2['OrderDt'], '%Y-%m-%d %H:%M:%S') + timedelta(days=odata2['Actual_V_StoreLT'] ) \n\n    # odata2['ReceivedDt'] = odata2['ReceivedDt'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n    # odata2['ReceivedDt'] = odata2['ReceivedDt'] + timedelta(days=odata2['Actual_V_StoreLT'])\n\n    # Convert to datetime (handles errors more gracefully)\n    odata2['OrderDt'] = pd.to_datetime(odata2['OrderDt'], errors='coerce')\n\n    # Ensure 'Actual_V_StoreLT' is numeric\n    odata2['Actual_V_StoreLT'] = pd.to_numeric(odata2['Actual_V_StoreLT'], errors='coerce')\n\n    # # Add timedelta safely\n\n    # Add timedelta vectorized\n    odata2['ReceivedDt'] = odata2['OrderDt'] + pd.to_timedelta(odata2['Actual_V_StoreLT'], unit='D')\n    \n    # To make it simple, we will assume for now - that we will receive everything that we order\n    odata2 = odata2.rename(columns={'OrderQty':'ReceivedQty'})\n    \n    # dminus5 = datetime.strptime(sdt, '%Y-%m-%d %H:%M:%S') + timedelta(days=-5)\n\n    \n    # However, we can't sell more than what we have ininventory so set sales to min of fc, and sales calculated above\n    odata3 = []\n    odata3 = odata2\n\n    \n    data = []\n    data = odata3[['order_id','Item_id','Store_id','ReceivedDt','ReceivedQty']]\n    Receiptsdata = []\n    \n    Receiptsdata = data[data['ReceivedDt']<=dt]\n    Receiptsdata = data[data['ReceivedDt'].notnull()]\n    \n    print(f\"ðŸš¨ Entering function_name with params: x={dt}, y={ItVdcST_LTOrdRecD_data}\")\n    breakpoint()\n    \n    row_count_no_null = Receiptsdata.dropna().shape[0]\n    if row_count_no_null != 0:\n        \n        # print('data from record sales fn')\n        # print(odata2)\n        if debug:\n            print(row_count_no_null)\n            print(Receiptsdata)\n        db_name = 'SupplyChainABC2.db'\n        table_name = 'StoreReceipts'\n        insert_dataframe_to_sqlite(Receiptsdata, db_name, table_name)\n        execute_query(\"select * from \" + table_name)\n        \n\n    \n\n    # return(data) \n    \n#- return statement is not mandotory\n    \n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.685964Z","iopub.execute_input":"2025-04-20T04:32:22.686221Z","iopub.status.idle":"2025-04-20T04:32:22.711168Z","shell.execute_reply.started":"2025-04-20T04:32:22.686203Z","shell.execute_reply":"2025-04-20T04:32:22.710185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Receiptsdata","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.712247Z","iopub.execute_input":"2025-04-20T04:32:22.713218Z","iopub.status.idle":"2025-04-20T04:32:22.731855Z","shell.execute_reply.started":"2025-04-20T04:32:22.713192Z","shell.execute_reply":"2025-04-20T04:32:22.730917Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6c. Extract Data and Record Inventory on nth Day**","metadata":{}},{"cell_type":"code","source":"# This is used for BOD and EOD InventoryOH calculation on nth day except first\n# fn cn:sqlite3.Connection in fn below specifies that the input is of type cn:sqlite3.Connection.  The -> pd.DataFrame specifies that it must return a DataFrame\ndef extract_ItSTDminus1EODasBODdnSaleReceipts(sdt,ohdt, cn:sqlite3.Connection,debug=False)-> pd.DataFrame:\n    import sqlite3\n    import pandas as pd\n    \n    # Establish a connection to the SQLite database\n    # connection = sqlite3.connect('your_database.db')\n    \n    # Write the SQL query to retrieve the data\n\n    query1  = \"\"\"\n\n    Select \n        SI.Item_id,\n        SI.Store_id,\n        SI.Sales_dt,\n        SI.Sales,\n        SI.InventoryDt,\n        SI.Dminus1EODasBODQtyn,\n        SR.order_id, \n        SR.ReceivedDt,\n        SR.ReceivedQty\n\n     FROM   \n    \n    (SELECT \n        Sn.Item_id,\n        Sn.Store_id,\n        Sn.Sales_dt,\n        Sn.Sales,\n        In_1.InventoryDt,\n        In_1.Dminus1EODasBODQtyn\n\n        \n    FROM \n    (Select S.Item_id,S.Store_id, S.Sales_dt, S.Sales   \n        FROM \n        StoreSales S\n        where \n        S.Sales_dt = \"\"\"\n   \n    query2 = \"\"\" \n     ) Sn,\n     (Select Item_id,Store_id, InventoryDt, EODQty as Dminus1EODasBODQtyn  \n        FROM InventoryOH where InventoryDt = \"\"\"\n\n    query3 = \"\"\" \n     ) In_1\n\n    where \n    Sn.Item_id = In_1.Item_id and\n    Sn.Store_id = In_1.Store_id \n    ) SI \n    \n    LEFT JOIN\n    \n    (Select Item_id,Store_id, order_id, ReceivedDt, ReceivedQty \n        FROM StoreReceipts where ReceivedDt = \"\"\"\n\n    \n    query4 = \"\"\" \n     ) SR\n    \n    on \n    SI.Item_id = SR.Item_id and\n    SI.Store_id = SR.Store_id \n\n    --GROUP BY 1,2,3\n    --GROUP BY 1,2,3\n    ORDER BY 1,2 ;\n    \n    \"\"\"\n\n\n    \n    query = query1 + \"'\" + sdt + \"'\" + query2 + \"'\" + ohdt + \"'\" + query3 + \"'\" + sdt + \"'\" + query4\n    if debug:\n        print(query)\n    # Use pandas to read the data into a DataFrame\n    # df1 = pd.read_sql_query(query1, db_conn)\n    dfItSTDminus1EODasBODdnSaleReceipts = pd.read_sql_query(query, cn)\n    # Close the connection\n    # connection.close()\n    \n    # Now, 'df' contains the data from the SQLite table\n    return(dfItSTDminus1EODasBODdnSaleReceipts)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.732842Z","iopub.execute_input":"2025-04-20T04:32:22.733052Z","iopub.status.idle":"2025-04-20T04:32:22.7476Z","shell.execute_reply.started":"2025-04-20T04:32:22.733036Z","shell.execute_reply":"2025-04-20T04:32:22.746607Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Define Daily Process Functions - 3. RecordInventory\n# RecordDaynInventory(dt,ItSTDminus1EODasBODdnSaleReceipts)\n\ndef RecordDaynInventory(dt,ItSTDminus1EODasBODdnSaleReceipts,debug=False):\n    \n    # EOD = BOD - Sale + Receipt.  We assume sale is limited to BOD.  Receipt is at EOD\n\n    import random\n    np.random.seed(42)\n    # datacombined['Forecast'].max()\n    bodSaleRdata = []\n    \n    bodSaleRdata = ItSTDminus1EODasBODdnSaleReceipts[['Item_id','Store_id','Dminus1EODasBODQtyn','ReceivedQty','Sales_dt','Sales']]\n    print(type(bodSaleRdata))\n\n    bodSaleRdata['ReceivedQty'] = bodSaleRdata['ReceivedQty'].fillna(0)\n\n    bodSaleRdata = bodSaleRdata.rename(columns={'Dminus1EODasBODQtyn':'BODQty'})\n    bodSaleRdata = bodSaleRdata.rename(columns={'Sales_dt':'InventoryDt'}) # We are pulling yesterdays EOD Invetory as today's BOD Inventory \n    # and todays sales to calculate today's ending inventory so we need to use the sales date as the new inventory date\n\n    bodSaleRdata['EODQty'] = bodSaleRdata['BODQty'] - bodSaleRdata['Sales'] + bodSaleRdata['ReceivedQty']\n    \n    \n    \n    data = []\n    data = bodSaleRdata[['Item_id','Store_id','InventoryDt','BODQty','EODQty']]\n    # print('data from record sales fn')\n\n    if debug:\n        print(data)\n    db_name = 'SupplyChainABC2.db'\n    table_name = 'InventoryOH'\n    insert_dataframe_to_sqlite(data, db_name, table_name)\n    execute_query(\"select * from \" + table_name)[:10]\n    \n\n    \n\n    # return(data) #- return statement is not mandotory","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.748741Z","iopub.execute_input":"2025-04-20T04:32:22.748987Z","iopub.status.idle":"2025-04-20T04:32:22.768123Z","shell.execute_reply.started":"2025-04-20T04:32:22.748969Z","shell.execute_reply":"2025-04-20T04:32:22.76719Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**6c. Extract Data and Record Orders**","metadata":{}},{"cell_type":"code","source":"# This is to extract LT,FC, Orders, EOD OH, Table data to Create and record Orders\n# fn cn:sqlite3.Connection in fn below specifies that the input is of type cn:sqlite3.Connection.  The -> pd.DataFrame specifies that it must return a DataFrame\n\n# I don't need dt for this as I am only looking at orders not yet received\ndef extract_ItVdcST_LTOrdEODn_data(dt, cn:sqlite3.Connection, debug=False)-> pd.DataFrame:\n    import sqlite3\n    import pandas as pd\n    \n    # Establish a connection to the SQLite database\n    # connection = sqlite3.connect('your_database.db')\n    \n    # Write the SQL query to retrieve the data\n    # SVSLT - System Vendor store LT\n    \n    query1  = \"\"\"\n    \n    Select Distinct\n        VSIOHSF.Item_id,\n        VSIOHSF.Vendor_id,\n    \tVSIOHSF.DC_id,\n        VSIOHSF.Store_id,\n        VSIOHSF.Vendor_DC_LT, \n        VSIOHSF.DC_StoreLT,\n     \tVSIOHSF.SVSLT,\n        VSIOHSF.InventoryDt,\n        VSIOHSF.EODQty,\n        VSIOHSF.Forecast_sales_dt ,\n        VSIOHSF.Forecast,\n        OT.order_id,\n        OT.OrderDt,\n        OT.OrderQty\n\n    FROM\n    \n    (SELECT \n        I.Item_id,\n        V.Vendor_id,\n    \tV.DC_id,\n        S.Store_id,\n        V.Vendor_DC_LT, \n        S.DC_StoreLT,\n     \tV.Vendor_DC_LT + S.DC_StoreLT AS SVSLT,\n        OH.InventoryDt,\n        OH.EODQty,\n        SF.Forecast_sales_dt,\n        SF.Forecast\n        \n    FROM \n    VendorDC V,\n    StoreDC S,\n    VendorItem I,\n\n    (Select \n        Item_id,\n        Store_id, \n        InventoryDt, EODQty   \n        FROM InventoryOH where InventoryDt = \"\"\"\n   \n    query2 = \"\"\" \n     ) OH,\n    (Select Item_id,Store_id, Forecast, Forecast_sales_dt  \n        FROM StoreFC where Forecast_sales_dt = \"\"\"\n   \n    query3 = \"\"\" \n     ) SF\n    \n    where \n    V.DC_id = S.DC_id and\n    V.Vendor_id = I.Vendor_id and \n    OH.Store_id = S.Store_id and\n    OH.Item_id = I.Item_id  and\n    SF.Store_id = S.Store_id and\n    SF.Item_id = I.Item_id  \n\n    \n    ) AS VSIOHSF\n    \n    LEFT JOIN\n    \n    (SELECT \n        order_id,\n        OrderDt,\n        Item_id,\n        Vendor_id,\n        Store_id,\n        OrderQty\n    From\n        orders\n    where \n        order_id\n    NOT IN (\n            SELECT DISTINCT\n                order_id \n            FROM\n                StoreReceipts)\n                ) AS OT      \n    ON    \n  \tVSIOHSF.Item_id = OT.Item_id  and\n  \tVSIOHSF.Vendor_id = OT.Vendor_id  and\n    VSIOHSF.Store_id = OT.Store_id \n \n    \n    ORDER BY \n    VSIOHSF.Item_id,\n    VSIOHSF.Vendor_id,\n    VSIOHSF.DC_id,\n    VSIOHSF.Store_id;\n\n    \n    \"\"\"\n    # I am collecting orders not yet delivered here and then i will check if the individual combinations\n    # order is expected in the OUTL window\n    query = query1 + \"'\" + dt + \"'\" + query2 + \"'\" + dt + \"'\" + query3\n\n    \n    if debug:\n        print(query)\n    # Use pandas to read the data into a DataFrame\n    # df1 = pd.read_sql_query(query1, db_conn)\n    dfItVdcST_LTOrdEODn_data = pd.read_sql_query(query, cn)\n    # Close the connection\n    # connection.close()\n    \n   \n    return(dfItVdcST_LTOrdEODn_data)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.772895Z","iopub.execute_input":"2025-04-20T04:32:22.773182Z","iopub.status.idle":"2025-04-20T04:32:22.787556Z","shell.execute_reply.started":"2025-04-20T04:32:22.773161Z","shell.execute_reply":"2025-04-20T04:32:22.786588Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  Define Daily Process Functions - 4. CreateAndRecordOrders\ndef CreateAndRecordOrders(RT, SSdays,dt,ItVdcST_LTOrdEODn_data):\n    \n# Steps\n#     Caculate OUTL = (LT + RT + SSdays) * FC\n#     Calculate OP  = (LT + 0.5 * RT + SSdays) * FC\n#     OH = EODQty\n#     OO = Orderqty for all outstanding orders -  even if ordered today it is expected to be delivered within LT+RT window\n#     Calculate AvailableInv = OH + OO\n#     Rule: Create an Order = if AvailableInv < OP , (OUTL - AvailableInv) else 0\n\n    \n    import random\n    np.random.seed(42)\n    # datacombined['Forecast'].max()\n    odata = pd.DataFrame([])\n    \n    odata = ItVdcST_LTOrdEODn_data[['Item_id','Vendor_id','Store_id','OrderDt','OrderQty','SVSLT','InventoryDt', 'EODQty','Forecast']]\n    # odata.groupby([\"Item_id\",'Vendor_id', \"Store_id\"]).agg({\"OrderQty\": [\"sum\"]})\n    grouped = (\n    odata\n    .groupby([\"Item_id\", \"Vendor_id\", \"Store_id\"])\n    .agg({\"OrderQty\": \"sum\"})\n    .reset_index()\n    .rename(columns={\"OrderQty\": \"OO_Qty\"})\n    )\n\n    distinct_rows = odata.drop_duplicates(subset=[\"Item_id\", \"Vendor_id\", \"Store_id\"])\n\n    merged_odata = pd.merge(distinct_rows, grouped, on=[\"Item_id\", \"Vendor_id\", \"Store_id\"], how=\"left\")\n\n    \n    print(type(odata))\n    \n    merged_odata['OUTL'] = (merged_odata['SVSLT'] + RT + SSdays) * merged_odata['Forecast']\n    merged_odata['OP'] = (merged_odata['SVSLT'] + 0.5* RT + SSdays) * merged_odata['Forecast']\n\n    merged_odata['OP'] = merged_odata['OP'].round(0).astype(int) \n\n    \n    merged_odata = merged_odata.rename(columns={'EODQty':'OHQty'})\n    merged_odata['AvailableInv'] = merged_odata['OHQty'] + merged_odata['OO_Qty']\n   \n\n    merged_odata[\"NewOrderQty\"] = np.where(merged_odata[\"AvailableInv\"] < merged_odata[\"OP\"], merged_odata[\"OUTL\"] - merged_odata[\"AvailableInv\"], 0)\n    merged_odata[\"NewOrderQty\"] = merged_odata[\"NewOrderQty\"].round(0).astype(int)\n    \n    odata2 = merged_odata[merged_odata[\"NewOrderQty\"] != 0]\n    odata3 = odata2[['InventoryDt','Item_id','Vendor_id','Store_id','NewOrderQty']]\n    odata3 = odata3.rename(columns={'InventoryDt':'OrderDt'})\n    odata3 = odata3.rename(columns={'NewOrderQty':'OrderQty'})\n\n    data = odata3[['OrderDt','Item_id','Vendor_id','Store_id','OrderQty']]\n    # Receiptsdata = []\n    # Receiptsdata = data[data['ReceivedDt'].notnull()]\n\n    row_count_no_null = data.dropna().shape[0]\n    if row_count_no_null != 0:\n        \n        # print('data from record sales fn')\n        # print(odata2)\n        print(\"New Order Count for day:\", dt,\"equals \",row_count_no_null)\n        print(data)\n        db_name = 'SupplyChainABC2.db'\n        table_name = 'orders'\n        insert_dataframe_to_sqlite(data, db_name, table_name)\n        execute_query(\"select * from \" + table_name)\n    \n\n    \n\n    # return(merged_odata) \n    \n#- return statement is not mandotory\n    \n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.788544Z","iopub.execute_input":"2025-04-20T04:32:22.788804Z","iopub.status.idle":"2025-04-20T04:32:22.807439Z","shell.execute_reply.started":"2025-04-20T04:32:22.788784Z","shell.execute_reply":"2025-04-20T04:32:22.806436Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Process daily ERP steps for the Planning Period\nimport datetime\nimport random\nfrom datetime import datetime, timedelta\n# Set debug mode\ndebug = True\n\nsold_out_dfs = []\n\ndate_obj = pd.to_datetime(planStartdt)         # Convert to Timestamp for adding/subtracting\nsecond_day = (date_obj + pd.Timedelta(days=1))      #Add -1 day\nday2 = second_day.strftime(\"%Y-%m-%d\") #formatted_string\n\n\ndaysToProcess = daysinplan -1\n# daysinplan = 30-1 # we want to use it to loop over all days from second day(excluding the initial day 1 processes )\ndatesList = pd.to_datetime(pd.date_range(start=day2, periods=daysToProcess, freq=\"D\")).strftime(\"%Y-%m-%d\")\n\n\n\n\n\n\n\nfor sdt in datesList:\n\n    date_obj = pd.to_datetime(sdt)         # Convert to Timestamp for adding/subtracting\n    dt_minus_one_day = (date_obj + pd.Timedelta(days=-1))      #Add -1 day\n    ohdt = dt_minus_one_day.strftime(\"%Y-%m-%d\") #formatted_string\n    # print(sdt,ohdt)\n    \n    \n    ###  1. Record Sales ###\n\n    # Extracting data from different tables for simulating sales (day 2 to end) as a normal distribution,\n    # assuming FC as the mean and a stdev selected \n    dfItSTDminus1EODasBODdnFC = extract_ItSTDminus1EODasBODdnFC(sdt,ohdt, db_conn)\n\n    # Simulate and record sales in the sales table\n    soldout_df = Recordsales(dt, dfItSTDminus1EODasBODdnFC)\n    sold_out_dfs.append(soldout_df)\n    print (\"recorded sales for day: \", sdt)\n    \n    #Used in the initial inventory calculation - may not be needed here\n    # ItVdcST_LTFCSalesdata = extract_ItVdcST_LTFCSalesdata(dt, db_conn)\n    \n    # ### 2. RecordReceipts from Day 2 to end ###\n\n    ItVdcST_LTOrdRecD_data = extract_ItVdcST_LTOrdRecD_data(db_conn) # extracts data\n    # ItVdcST_LTOrdRecD_data\n    row_count_no_null = ItVdcST_LTOrdRecD_data.dropna().shape[0] # check if there are any orders \n    # not yet recorded in the receipts table but received today or before \n    # row_count_no_null = 1 # this is to test - disable later\n\n    print (\"reviewed orders for day: \", sdt, \"there was \", row_count_no_null, \" outstanding orders to check for receipts.\" )\n\n    if row_count_no_null !=0:\n        print('running RecordReceipts' )\n        #Receiptsdata = RecordReceipts(dt,ItVdcST_LTOrdRecD_data)\n        RecordReceipts(sdt,ItVdcST_LTOrdRecD_data)\n        print('finished running RecordReceipts' )\n        # extract LT, Orders Not in Received Table\n        # compute simulated VDCLT and DCSLT add to order date to calc Received dt\n        # If Recd date <= dt (dt we are processing) add it to Recd table\n        print (\"recorded receipts for day: \", sdt)\n    \n\n    ### 3. Extract data and  Record Inventory - BOD OH = EOD OH for Prev day; EOD OH = BOD - Sale + Receipts  ####\n\n    ItSTDminus1EODasBODdnSaleReceipts = extract_ItSTDminus1EODasBODdnSaleReceipts(sdt,ohdt, db_conn)\n    ItSTDminus1EODasBODdnSaleReceipts\n    \n    RecordDaynInventory(sdt,ItSTDminus1EODasBODdnSaleReceipts)\n    print (\"recorded inventory for day: \", sdt)\n\n    ### 4. Extract data and Create and Record Orders  ####\n\n    ItVdcST_LTOrdEODn_data = extract_ItVdcST_LTOrdEODn_data(sdt, db_conn)\n    print (\"extracted data for order creation for day: \", sdt)\n    \n    RT = 1 # Since we are reviewing everyday\n    SSdays = 1 # Safety Stock # of Days of supply\n    \n    CreateAndRecordOrders(RT, SSdays,sdt,ItVdcST_LTOrdEODn_data)\n    print (\"recorded orders for day: \", sdt)\n    \n    # if sdt >= '2025-01-24':\n    #     if debug:\n    #         user_input = input(\"âž¡ï¸ Press Enter to continue, or type 'q' to quit: \")\n    #         if user_input.lower() == 'q':\n    #             print(\"ðŸ›‘ Loop terminated by user.\")\n    #             break\n\n# dfSalesAndInv\n\n###  Backup db and close connections\n# Open a connection to a file-based database\nbackup_conn = sqlite3.connect(\"/kaggle/working/SupplyChainBackup.db\")\n\n# Copy data from in-memory to file\nwith backup_conn:\n    db_conn.backup(backup_conn)\n\n# Close both connections\nbackup_conn.close()\ndb_conn.close()\n\n\n\n# Loop over all days to process all these steps\n","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:22.808561Z","iopub.execute_input":"2025-04-20T04:32:22.808836Z","iopub.status.idle":"2025-04-20T04:32:24.802657Z","shell.execute_reply.started":"2025-04-20T04:32:22.80881Z","shell.execute_reply":"2025-04-20T04:32:24.801608Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Combine all DataFrames at once\ncombined_sold_out = pd.concat(sold_out_dfs, ignore_index=True)\n\n# Remove duplicates if needed\ncombined_sold_out = combined_sold_out.drop_duplicates()\n\nprint(f\"Combined {len(sold_out_dfs)} DataFrames into {len(combined_sold_out)} records\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:37:30.120106Z","iopub.execute_input":"2025-04-20T04:37:30.12044Z","iopub.status.idle":"2025-04-20T04:37:30.129311Z","shell.execute_reply.started":"2025-04-20T04:37:30.120415Z","shell.execute_reply":"2025-04-20T04:37:30.128543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"combined_sold_out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nprint(sys.version)","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:32:24.803886Z","iopub.execute_input":"2025-04-20T04:32:24.804176Z","iopub.status.idle":"2025-04-20T04:32:24.808527Z","shell.execute_reply.started":"2025-04-20T04:32:24.804155Z","shell.execute_reply":"2025-04-20T04:32:24.807648Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# At the (Pdb) prompt, you can now type:\n# \tâ€¢\tprint(df.head()) â†’ View data\n# \tâ€¢\tc â†’ Continue\n# \tâ€¢\tq â†’ Quit\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:32:24.809525Z","iopub.execute_input":"2025-04-20T04:32:24.810097Z","iopub.status.idle":"2025-04-20T04:32:24.82597Z","shell.execute_reply.started":"2025-04-20T04:32:24.81007Z","shell.execute_reply":"2025-04-20T04:32:24.825175Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extra helper Functions","metadata":{}},{"cell_type":"code","source":"db_file = \"SupplyChainABC2.db\"\n\n\n\ndb_conn = sqlite3.connect(db_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:39:07.026698Z","iopub.execute_input":"2025-04-20T04:39:07.027009Z","iopub.status.idle":"2025-04-20T04:39:07.031547Z","shell.execute_reply.started":"2025-04-20T04:39:07.026987Z","shell.execute_reply":"2025-04-20T04:39:07.030715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def list_tables() -> list[str]:\n    \"\"\"Retrieve the names of all tables in the database.\"\"\"\n    # Include print logging statements so you can see when functions are being called.\n    print(' - DB CALL: list_tables()')\n\n    cursor = db_conn.cursor()\n\n    # Fetch the table names.\n    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n\n    tables = cursor.fetchall()\n    return [t[0] for t in tables]\n\n\nlist_tables()","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:39:10.605894Z","iopub.execute_input":"2025-04-20T04:39:10.606493Z","iopub.status.idle":"2025-04-20T04:39:10.614524Z","shell.execute_reply.started":"2025-04-20T04:39:10.606465Z","shell.execute_reply":"2025-04-20T04:39:10.613632Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def describe_table(table_name: str) -> list[tuple[str, str]]:\n    \"\"\"Look up the table schema.\n\n    Returns:\n      List of columns, where each entry is a tuple of (column, type).\n    \"\"\"\n    print(f' - DB CALL: describe_table({table_name})')\n\n    cursor = db_conn.cursor()\n\n    cursor.execute(f\"PRAGMA table_info({table_name});\")\n\n    schema = cursor.fetchall()\n    # [column index, column name, column type, ...]\n    return [(col[1], col[2]) for col in schema]\n\n\ndescribe_table(\"StoreSales\")","metadata":{"execution":{"iopub.status.busy":"2025-04-20T04:39:15.592812Z","iopub.execute_input":"2025-04-20T04:39:15.593118Z","iopub.status.idle":"2025-04-20T04:39:15.601822Z","shell.execute_reply.started":"2025-04-20T04:39:15.593095Z","shell.execute_reply":"2025-04-20T04:39:15.600939Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"describe_table(\"Store\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:42:07.151004Z","iopub.execute_input":"2025-04-20T04:42:07.151858Z","iopub.status.idle":"2025-04-20T04:42:07.158322Z","shell.execute_reply.started":"2025-04-20T04:42:07.151829Z","shell.execute_reply":"2025-04-20T04:42:07.157543Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"describe_table(\"Item\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:42:18.652545Z","iopub.execute_input":"2025-04-20T04:42:18.652842Z","iopub.status.idle":"2025-04-20T04:42:18.659774Z","shell.execute_reply.started":"2025-04-20T04:42:18.65282Z","shell.execute_reply":"2025-04-20T04:42:18.658933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def enrich_sold_out_data(db_conn, combined_sold_out):\n    \"\"\"\n    Enrich sold-out data with store/item names, treating (Store_id, Item_id, Sales_dt) as unique key\n    Returns DataFrame with columns:\n    [Store_id, Store_name, StoreCity, Item_id, Item_name, price, Sales_dt, Sales]\n    \"\"\"\n    # Convert dates to strings for SQL querying\n    combined_sold_out['Sales_dt_str'] = combined_sold_out['Sales_dt'].astype(str)\n    \n    # 1. Get unique combinations for efficient querying\n    unique_keys = combined_sold_out[['Store_id', 'Item_id', 'Sales_dt_str']].drop_duplicates()\n    \n    # 2. Query store information (many-to-one relationship)\n    #store_ids = tuple(unique_keys['Store_id'].unique())\n    store_query = f\"\"\"\n    SELECT Store_id, Store_name, StoreCity \n    FROM Store\n    \"\"\"\n    stores_df = pd.read_sql(store_query, db_conn)\n    \n    # 3. Query item information (many-to-one relationship)\n    #item_ids = tuple(unique_keys['Item_id'].unique())\n    item_query = f\"\"\"\n    SELECT Item_id, Item_name, price\n    FROM Item\n    \"\"\"\n    items_df = pd.read_sql(item_query, db_conn)\n    \n    # 5. Merge all information\n    enriched_df = (\n        combined_sold_out\n        .merge(stores_df, on='Store_id', how='left')\n        .merge(items_df, on='Item_id', how='left')\n    )\n    \n    \n    \n    # 7. Final column ordering\n    column_order = [\n        'Store_id', 'Store_name', 'StoreCity',\n        'Item_id', 'Item_name', 'price',\n        'Sales_dt', 'day_of_week', 'Sales'  # Added temporal dimension\n    ]\n    \n    return enriched_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:52:26.289801Z","iopub.execute_input":"2025-04-20T04:52:26.290133Z","iopub.status.idle":"2025-04-20T04:52:26.296741Z","shell.execute_reply.started":"2025-04-20T04:52:26.290077Z","shell.execute_reply":"2025-04-20T04:52:26.295809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enriched_df = enrich_sold_out_data(db_conn, combined_sold_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:57:52.195877Z","iopub.execute_input":"2025-04-20T04:57:52.196225Z","iopub.status.idle":"2025-04-20T04:57:52.213746Z","shell.execute_reply.started":"2025-04-20T04:57:52.196198Z","shell.execute_reply":"2025-04-20T04:57:52.212649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r SupplyChainABC2.db.zip /kaggle/working/SupplyChainABC2.db\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:03.625568Z","iopub.execute_input":"2025-04-20T04:54:03.625887Z","iopub.status.idle":"2025-04-20T04:54:03.876997Z","shell.execute_reply.started":"2025-04-20T04:54:03.625861Z","shell.execute_reply":"2025-04-20T04:54:03.875735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'SupplyChainABC2.db.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:54:07.345797Z","iopub.execute_input":"2025-04-20T04:54:07.346144Z","iopub.status.idle":"2025-04-20T04:54:07.352934Z","shell.execute_reply.started":"2025-04-20T04:54:07.346115Z","shell.execute_reply":"2025-04-20T04:54:07.352099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"enriched_df.to_csv('all_sold_out_events.csv', index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:58:01.089886Z","iopub.execute_input":"2025-04-20T04:58:01.09017Z","iopub.status.idle":"2025-04-20T04:58:01.10773Z","shell.execute_reply.started":"2025-04-20T04:58:01.090148Z","shell.execute_reply":"2025-04-20T04:58:01.106786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!zip -r all_sold_out_events.csv.zip /kaggle/working/all_sold_out_events.csv\nFileLink(r'all_sold_out_events.csv.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T04:58:49.631394Z","iopub.execute_input":"2025-04-20T04:58:49.6317Z","iopub.status.idle":"2025-04-20T04:58:49.763805Z","shell.execute_reply.started":"2025-04-20T04:58:49.631678Z","shell.execute_reply":"2025-04-20T04:58:49.762757Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}